{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageCLEF Medical Caption Task 2019\n",
    "\n",
    "The [Image CLEF 2019 Concept Detection Task](https://www.imageclef.org/2019/medical/caption/) is a large-scale multi-label classification task aiming to identify medical terms (concepts) in radiology images. Implement a system to classify a medical image based on several abnormalities represented by [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html) concept IDs.\n",
    "\n",
    "The AUEB NLP Group won the competition with [this paper](http://nlp.cs.aueb.gr/pubs/paper_136.pdf). This assignment was prepared by Vasiliki Kougia and John Pavlopoulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "* The training data exist in Google Drive.\n",
    "\n",
    "* You can download the training and validation data from <https://drive.google.com/uc?id=1UOccw0VNCiRTwaQSEJMhiYWXhizvKptX> and the test data from <https://drive.google.com/uc?id=1diO2apPPFJeTH8CGcd3S55OUNTXtJVu2>. Alternatively, you can use the [gdown](https://github.com/wkentaro/gdown) utility; the file IDs are `1UOccw0VNCiRTwaQSEJMhiYWXhizvKptX` and `1diO2apPPFJeTH8CGcd3S55OUNTXtJVu2`.\n",
    "\n",
    "* The data are organised as follows:\n",
    "\n",
    "  * `training-set`: 56,629 training images.\n",
    "  \n",
    "  * `validation-set`: 14,157 validation images.\n",
    "\n",
    "  * `Last`; rename it to `test-set`: 10,000 images used for testing. The test images have no annotations. They will be used for assessment.\n",
    "  \n",
    "  * `train_concepts.csv`: the image IDs of the training set with their gold (i.e., known correct) tags, separated with `;`.\n",
    "  \n",
    "  * `val_concepts.csv`: the validation image IDs with their gold tags, separated with `;`.\n",
    "\n",
    "  * `string_concepts.csv`: all the available tag IDs and their corresponding name, separated with tabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "* Explore your data.\n",
    "\n",
    "* Plot some images.\n",
    "\n",
    "* For those images, fetch their tag IDs and their tag names.\n",
    "\n",
    "* How many tags are there in total?\n",
    "\n",
    "* Which ones are the most frequent?\n",
    "\n",
    "* How many tags are there per image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* Preprocess the images so that you can use them as input.\n",
    "\n",
    "* You may have to preprocess the labels as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline\n",
    "\n",
    "* Think of a baseline classifier that you could use to to measure your efforts.\n",
    "\n",
    "* That could be a classifier that produces always the most frequent labels.\n",
    "\n",
    "* Alternatively (and probably better), it could be a classifier that samples from the labels based on their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network\n",
    "\n",
    "* You can use any of the neural network architectures you have seen in class, or any other architecture you may like.\n",
    "\n",
    "* You are free to try pretrained models. However, be warned that they may demand considerable resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "* For each validation image, measure the [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) of the predicted tags. You can use the evaluation code of the competition, which can be found in the official site, in the Evaluation Methodology section of the [competition web page](https://www.imageclef.org/2019/medical/caption/). Actually that's probably the best course of action to avoid getting bogged down in differences in F1 score implementations.\n",
    "\n",
    "* Calculate the average for all the *validation* images.\n",
    "\n",
    "* Keep in mind that the best F1 score achieved in the competition is 0.282 for the *test* images.\n",
    "\n",
    "* If nothing else, you can read the [paper](http://nlp.cs.aueb.gr/pubs/paper_136.pdf) to get the outline of possible solutions (but you can try your own, you do not have, and are not expected to, mimic previous work).\n",
    "\n",
    "* After finessing your model on the validation images, you will use it on the test images.\n",
    "\n",
    "* You must submit:\n",
    "\n",
    "  * Your notebook, indicating the score you achieved on the validation set.\n",
    "\n",
    "  * A file with your solutions on the test set, as explained in the Submission Instructions section of the competition web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code\n",
    "\n",
    "You understand that this is an individual assignment, and as such you must carry it out alone. You may seek help on the Internet, by Googling or searching in StackOverflow for general questions pertaining to the use of Python and pandas libraries and idioms. However, it is not right to ask direct questions that relate to the assignment and where people will actually solve your problem by answering them. You may discuss with your fellow students in order to better understand the questions, if they are not clear enough, but you should not ask them to share their answers with you, or to help you by giving specific advice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
